---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Optimizer

## Gradient Descent
### Stochastic Gradient Descent (SGD)
### Mini-batch Gradient Descent
### Batch Gradient Descent

## Momentum
### Momentum
### Nesterov Accelerated Gradient (NAG)

## Adaptive Learning Rate
### Adagrad (Adaptive Gradient Algorithm)
### Adadelta
### RMSprop (Root Mean Square Propagation)
### Adam (Adaptive Moment Estimation)
### AdamW (Adam with Weight Decay)
### Nadam (Nesterov-accelerated Adaptive Moment Estimation)
### AdaMax
### AMSGrad

## Second-Order
### L-BFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno)
### Hessian-Free Optimization

## Evolutionary
### Genetic Algorithms
### Particle Swarm Optimization (PSO)
### Differential Evolution (DE)
### Covariance Matrix Adaptation Evolution Strategy (CMA-ES)

## Misc
### FTRL (Follow The Regularized Leader)
### Yogi Optimizer
### RAdam (Rectified Adam)
### Lookahead Optimizer

# Scheduler
## Basic
### Step Decay
### Exponential Decay
### Polynomial Decay

## Advanced
### Cyclical Learning Rate (CLR)
### One Cycle Policy
### Cosine Annealing
### ReduceLROnPlateau

## Warmup
### Linear Warmup
### Exponential Warmup
### Gradual Warmup