{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04569cb4",
   "metadata": {},
   "source": [
    "# Data\n",
    "2 Types of data pipelines:\n",
    "- **ETL**: Extract → Transform → Load\n",
    "- **ELT**: Extract → Load → Transform\n",
    "\n",
    "This guide focuses on ETL basics.\n",
    "\n",
    "## Chapter 8: Feature Engineering and Data Preprocessing\n",
    "- 8.1 Introduction to Feature Engineering\n",
    "- 8.2 Data Cleaning\n",
    "  - 8.2.1 Handling Missing Data\n",
    "  - 8.2.2 Outlier Detection and Treatment\n",
    "- 8.3 Feature Scaling\n",
    "  - 8.3.1 Normalization\n",
    "  - 8.3.2 Standardization\n",
    "- 8.4 Feature Selection Techniques\n",
    "  - 8.4.1 Filter Methods\n",
    "  - 8.4.2 Wrapper Methods\n",
    "  - 8.4.3 Embedded Methods\n",
    "- 8.5 Feature Transformation\n",
    "  - 8.5.1 Polynomial Features\n",
    "  - 8.5.2 Interaction Features\n",
    "  - 8.5.3 Binning and Discretization\n",
    "  - 8.5.4 Encoding Categorical Variables\n",
    "- 8.6 Handling High-Dimensional Data\n",
    "- 8.7 Data Augmentation Techniques\n",
    "\n",
    "## Data Collection\n",
    "Sources:\n",
    "- **Internal**: Transaction logs, customer databases, sensor data, operational data.\n",
    "- **External**: Public datasets, third-party data, social media feeds, open-source platforms, user reviews.\n",
    "- **Synthetic**: Auto-generated data for simulation, used when real data is unavailable due to privacy/rarity.\n",
    "\n",
    "Procedure:\n",
    "1. Define requirements.\n",
    "2. Establish scalable infra.\n",
    "3. Ensure quality.\n",
    "4. Continuously monitor.\n",
    "\n",
    "Methods:\n",
    "- **Direct**: Surveys, observations, experiments, purchases\n",
    "    - Tailored for ML tasks, common in R&D.\n",
    "- **Indirect**: Scraping, database access, APIs\n",
    "    - Require preprocessing.\n",
    "- **Crowdsourcing**: Large group data generation\n",
    "    - Often used for annotations.\n",
    "\n",
    "Challenges: Volume, variety, veracity, velocity, ethics.\n",
    "\n",
    "## Data Cleaning\n",
    "Procedure:\n",
    "- **Handle missing data**:\n",
    "  - **Deletion**: When the proportion is negligible.\n",
    "  - **Imputation**: Applicable to randomly missing data.\n",
    "- **Remove unwanted samples/features**: Duplicates, irrelevant samples/features, etc.\n",
    "- **Fix structural errors**: Typos, mislabels, inconsistencies, etc.\n",
    "- **Filter outliers**: Use domain-specific filters or statistical methods (Z-score, IQR, etc.).\n",
    "  - Remove for non-robust models, keep for robust models.\n",
    "- **Handle text data**: Lowercase, punctuation, typos, stopwords, lemmatization, etc.\n",
    "- **Handle image data**:\n",
    "  - **Size**: Resizing, cropping, padding, etc.\n",
    "  - **Color**: Grayscale conversion, histogram equalization, color space transformation, etc.\n",
    "  - **Noise**: Gaussian blur, median blur, denoising, artifact removal.\n",
    "  - **File**: Ensure uniform format.\n",
    "\n",
    "**Challenges**: Scalability, unstructured data, information loss due to over-cleaning, etc.\n",
    "\n",
    "## Data Imputation\n",
    "Procedure (like EM):\n",
    "1. Estimate missing data.\n",
    "2. Estimate params for imputation.\n",
    "3. Repeat.\n",
    "\n",
    "Types:\n",
    "- **Simple**: Zero, majority, mean (usually best).\n",
    "  - Assumes no multicollinearity.\n",
    "- **Complex**:\n",
    "  - **Regression**: Fit missing features on other features, assumes multicollinearity.\n",
    "    - Cons: potential assumption failures.\n",
    "  - **Indicator addition**: Add 0-1 indicators for missing features.\n",
    "    - Cons: feature size doubles.\n",
    "  - **Category addition**: Add \"missing\" category for missing values.\n",
    "    - Pros: straightforward, better than doubling.\n",
    "  - **Unsupervised Learning**: Used if many categories/features.\n",
    "\n",
    "## Data Transformation\n",
    "### Standardization\n",
    "$$X_\\text{new}=\\frac{X-\\bar{X}}{\\Sigma_X}$$\n",
    "\n",
    "Pros:\n",
    "- Removes mean & scales data to unit variance (i.e., $ x_i\\sim N(0,1)$).\n",
    "\n",
    "Cons:\n",
    "- Sensitive to outliers (because they affect empirical mean & std).\n",
    "- Destroys sparsity (because center is shifted).\n",
    "\n",
    "### Normalization\n",
    "$$X_\\text{new}=\\frac{X}{\\text{norm}(X)}$$\n",
    "\n",
    "Pros:\n",
    "- Scales samples to unit norms.\n",
    "- Supports L1/L2/max norms.\n",
    "\n",
    "### Min-Max Scaling\n",
    "$$\\begin{align*}\n",
    "&x\\in[0,1]: &&X_\\text{new}=\\frac{X-\\min{(X)}}{\\max{(X)}-\\min{(X)}}\\\\\n",
    "&x\\in[\\min,\\max]: &&X_\\text{new}=\\frac{X-\\min{(X)}}{\\max{(X)}-\\min{(X)}}(\\text{max}-\\text{min})+\\text{min}\n",
    "\\end{align*}$$\n",
    "\n",
    "Pros:\n",
    "- Scales data to a customizable range.\n",
    "\n",
    "Cons:\n",
    "- Sensitive to outliers (because they affect empirical min & max).\n",
    "- Destroys sparsity (because center is shifted).\n",
    "\n",
    "### Max-Abs Scaling\n",
    "$$X_\\text{new}=\\frac{X}{\\max{(|X|)}}$$\n",
    "\n",
    "Pros:\n",
    "- Preserves signs.\n",
    "- Preserves sparsity.\n",
    "- Scales data to $[-1,1]$.\n",
    "\n",
    "Cons:\n",
    "- Sensitive to outliers.\n",
    "\n",
    "### Robust Scaling\n",
    "$$X_\\text{new}=\\frac{X-\\text{med}(X)}{Q_{75\\%}(X)-Q_{25\\%}(X)}$$\n",
    "\n",
    "Pros:\n",
    "- Robust to outliers\n",
    "\n",
    "Cons:\n",
    "- Destroys sparsity (because center is shifted).\n",
    "\n",
    "### Quantile Transform\n",
    "- Original: $X_\\text{new}=Q^{-1}(F(X))$\n",
    "    - $Q^{-1}$: Quantile function (i.e., PPF, inverse of CDF).\n",
    "    - $F$: Empirical CDF.\n",
    "- Uniform: $X_\\text{new}=F_U^{-1}(F(X))\\in[0,1]$\n",
    "- Gaussian: $X_\\text{new}=F_N^{-1}(F(X))\\sim N(0,1)$\n",
    "\n",
    "Pros:\n",
    "- Robust to outliers (by collapsing them).\n",
    "\n",
    "Cons:\n",
    "- Distorts linear correlations between diff features.\n",
    "- Requires large #samples.\n",
    "\n",
    "\n",
    "### Power Transform\n",
    "- Yeo-Johnson Transform\n",
    "\n",
    "    $$\n",
    "    \\mathbf{x}_i^{(\\lambda)}=\\begin{cases}\n",
    "    \\frac{(\\mathbf{x}_i+1)^\\lambda-1}{\\lambda} & \\text{if }\\lambda\\neq0,\\mathbf{x}_i\\geq0 \\\\\n",
    "    \\ln{(\\mathbf{x}_i+1)}                      & \\text{if }\\lambda=0,\\mathbf{x}_i\\geq0 \\\\\n",
    "    \\frac{1-(1-\\mathbf{x}_i)^{2-\\lambda}}{2-\\lambda} & \\text{if }\\lambda\\neq2,\\mathbf{x}_i<0 \\\\\n",
    "    -\\ln{(1-\\mathbf{x}_i)}                           & \\text{if }\\lambda=2,\\mathbf{x}_i<0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "    - $\\lambda$: Determined by MLE.\n",
    "\n",
    "- Box-Cox Transform\n",
    "\n",
    "    $$\n",
    "    \\mathbf{x}_i^{(\\lambda)}=\\begin{cases}\n",
    "    \\frac{\\mathbf{x}_i^\\lambda-1}{\\lambda} & \\text{if }\\lambda\\neq0 \\\\\n",
    "    \\ln{(\\mathbf{x}_i)} & \\text{if }\\lambda=0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "    - Requires $\\mathbf{x}_i>0$.\n",
    "\n",
    "Pros:\n",
    "- Maps data to Gaussian distribution (stabilizes variance & minimizes skewness)\n",
    "- Useful against heteroskedasticity.\n",
    "- Sklearn's PowerTransformer converts data to $N(0,1)$ by default.\n",
    "\n",
    "Cons:\n",
    "- Distorts linear correlations between diff features.\n",
    "\n",
    "### Categorical features\n",
    "- **One-Hot Encoding**: Converts each category into a 0-1 feature, better for nominal data.\n",
    "- **Label Encoding**: Converts each category into a numerical label, better for ordinal data.\n",
    "\n",
    "## Data Loading\n",
    "Loading data IRL is more complex than school projects.\n",
    "\n",
    "Procedure:\n",
    "1. **Choose Storage**:\n",
    "    - **Databases**: SQL (relational, structured), NoSQL (unstructured).\n",
    "    - **Data warehouses**: Ideal for analytical tasks.\n",
    "    - **Data lakes**: Store raw big data from various sources.\n",
    "    - **Cloud storage**\n",
    "2. **Validate**: Check schema, data quality, integrity, etc.\n",
    "3. **Format**: Ensure proper encoding, batching (for big data), raw saving.\n",
    "4. **Load**:\n",
    "    - **Bulk Loading**: Load large data chunks\n",
    "        - Minimizes logging and transaction overhead.\n",
    "        - Requires system downtime.\n",
    "    - **Incremental Loading**: Load data in small increments\n",
    "        - Uses timestamps/logs to track changes.\n",
    "        - Minimizes disruption.\n",
    "        - Ideal for real-time processing.\n",
    "    - **Streaming**: Load data continuously in real-time.\n",
    "5. **Optimize**: Reduce data volume for faster execution.\n",
    "    - **[Indexing](#indexing)**: Use primary/secondary indexes for faster data retrieval. \n",
    "        - Best for tables with low data churn.\n",
    "    - **[Partitioning](#sharding-horizontal-partitioning)**: Divide databases/tables for independent querying.\n",
    "        - Best for older records.\n",
    "    - **[Parallel Processing](#parallel-processing)**\n",
    "6. **Handle Errors**\n",
    "7. **Ensure Security**: Encryption, access control, etc.\n",
    "8. **Verify**: Audit with test queries, reconcile loaded data with source data, etc.\n",
    "\n",
    "### Indexing\n",
    "What: Create quick lookup paths using B trees/B+ trees for faster data retrieval.\n",
    "\n",
    "Types:\n",
    "- **Single-column**: For frequent access/filtering of one column.\n",
    "- **Composite**: For frequent filtering/sorting based on multiple columns.\n",
    "- **Unique**: Ensures all index values are unique, used as primary key.\n",
    "- **Full-text**: For complex queries on unstructured texts.\n",
    "- **Spatial**: For geospatial operations.\n",
    "\n",
    "Pros: Fast retrieval, automatic sorting, high time efficiency.\n",
    "\n",
    "Cons: Low space efficiency, high maintenance complexity.\n",
    "\n",
    "### Sharding (Horizontal Partitioning)\n",
    "What: Distribute data across multiple servers/locations using a shard key for horizontal database scaling.\n",
    "\n",
    "Types:\n",
    "- **Hash-based**: Even data division using a hash function.\n",
    "- **Range-based**: Numerical data division using key value ranges.\n",
    "- **List-based**: Categorical data division using predefined shard key lists.\n",
    "\n",
    "Uses: Web apps, real-time analytics, game/media services, etc.\n",
    "\n",
    "Pros: Horizontal scalability, high availability.\n",
    "\n",
    "Cons: Implementation & maintenance complexity.\n",
    "\n",
    "### Parallel Processing\n",
    "Parallel processing runs on a single computer (node).\n",
    "\n",
    "Concepts:\n",
    "- **Core**: Independent instruction execution units in a processor.\n",
    "- **Thread**: Sub-tasks run independently on a core.\n",
    "- **Memory**: Shared or distributed.\n",
    "\n",
    "Types:\n",
    "- **Data Parallelism**: Process data chunks simultaneously on different cores.\n",
    "- **Task Parallelism**: Execute different tasks in parallel.\n",
    "\n",
    "Methods:\n",
    "- **Multithreading**: Use libraries like C/C++ OpenMP, Python threading, etc.\n",
    "- **GPGPU**: Use CUDA/OpenCL for efficient parallel computing.\n",
    "\n",
    "### Distributed Computing\n",
    "Distributed Computing runs on multiple independent computers (nodes).\n",
    "\n",
    "Concepts:\n",
    "- **Networks**: Connect multiple computers.\n",
    "- **Horizontal Scalability**: Performance improves with more machines.\n",
    "- **Fault Tolerance**: Handle node/network failures without affecting the overall task.\n",
    "\n",
    "**Methods**:\n",
    "- **MapReduce**: Process big data with distributed algorithms.\n",
    "    - **Map**: Filter and sort data.\n",
    "    - **Reduce**: Summarize data.\n",
    "- **Distributed Databases**: Store data across multiple locations but appear as a single database.\n",
    "- **Load Balancing**: Evenly distribute workloads to maximize resource usage and minimize response time."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}