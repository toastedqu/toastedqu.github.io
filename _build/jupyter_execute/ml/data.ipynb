{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97481f0c",
   "metadata": {},
   "source": [
    "# Data\n",
    "Data pipelines in ML follow either one below:\n",
    "- **ETL**: Extract $\\rightarrow$ Transform $\\rightarrow$ Load\n",
    "- **ELT**: Extract $\\rightarrow$ Load $\\rightarrow$ Transform\n",
    "\n",
    "This page follows ETL and only covers the basics because I'm not a data engineer.\n",
    "\n",
    "## Data Collection\n",
    "Sources:\n",
    "- **Internal**: transaction logs, customer databases, sensor data, operational data, etc.\n",
    "- **External**: public datasets, third-party purchases, social media feeds, open-source platforms, user reviews, etc.\n",
    "- **Synthetic**: auto-generated data to simulate real world; used when real data are unavailable due to privacy concerns/rarity\n",
    "\n",
    "Procedure:\n",
    "1. Define requirements\n",
    "2. Establish scalable infra\n",
    "3. Quality assurance\n",
    "4. Continuous monitoring\n",
    "\n",
    "Methods:\n",
    "- **Direct**: surveys, observations, experiments, purchases, etc.; tailored for ML tasks; commonly used in R&D\n",
    "- **Indirect**: scraping, database access, APIs, etc.; require preprocessing\n",
    "- **Crowdsourcing**: gather a large group of people to generate data; commonly used for annotations\n",
    "\n",
    "Challenges: volume, variety, veracity, velocity, ethics, etc.\n",
    "\n",
    "\n",
    "\n",
    "## Data Cleaning\n",
    "Procedure (unordered):\n",
    "- **Handle missing data**: either at random/not at random\n",
    "    - **Deletion** (only when the proportion is negligible)\n",
    "    - [**Imputation**](#imputation) (only applicable to random missing, likely non-factual)\n",
    "- **Remove unwanted samples/features**: duplicates, irrelevant samples/features, etc.\n",
    "- **Fix structural errors**: typos, mislabels, inconsistency, etc.\n",
    "- **Filter unwanted outliers**: statistical methods (Z-score, IQR [interquartile range], etc.), domain-specific filters\n",
    "    - remove if non-robust model, keep if robust model\n",
    "- **Handle text data**: lowercase, punctuations, typos, stopwords, lemmatization (reduce word to root form), etc.\n",
    "- **Handle image data**: \n",
    "    - **Size**: resizing, cropping, padding, etc.\n",
    "    - **Color**: grayscale conversion, histogram equalization (enhance contrast), color space transformation (e.g., RGB $\\rightarrow$ HSV)\n",
    "    - **Noise**: Gaussian blur, median blur, denoising, artifact remmoval (e.g., text overlays, watermarks, etc.)\n",
    "    - **File**: ensure uniform file format\n",
    "\n",
    "Challenges: scalability, unstructured data, info loss due to overcleaning, etc.\n",
    "\n",
    "### Data Imputation\n",
    "Procedure (like EM): \n",
    "1. Estimate the missing data.\n",
    "2. Estimate the params for imputation.\n",
    "3. Repeat.\n",
    "\n",
    "Types:\n",
    "- Simple imputation: zero, majority, mean (usually best)\n",
    "    - assume no multicollinearity\n",
    "- Complex imputation:\n",
    "    - **Regression**: fit missing feature on other features (assume multicollinearity)\n",
    "        - NOT necessarily better than simple imputations because assumptions can fail\n",
    "    - **Indicator addition**: add 0-1 indicators for each feature on whether this feature is missing (0: present; 1: absent)\n",
    "        - treat the fact \"missing\" as an informative value\n",
    "        - Cons: doubled feature size\n",
    "    - **Category addition**: add one more category called \"missing\" to represent missing values\n",
    "        - treat the fact \"missing\" as an informative value\n",
    "        - Pros: straightforward & much better than doubled numerical values\n",
    "    - **Unsupervised Learning**: ussed if there are lots of categories and/or features\n",
    "\n",
    "\n",
    "\n",
    "## Data Transformation\n",
    "### Standardization\n",
    "$$X_\\text{new}=\\frac{X-\\bar{X}}{\\Sigma_X}$$\n",
    "\n",
    "Pros:\n",
    "- remove mean and/or scale data to unit variance (i.e., $ x_i\\sim N(0,1)$)\n",
    "\n",
    "Cons:\n",
    "- highly sensitive to outliers (because they greatly impact empirical mean & std)\n",
    "- destroy sparsity (because center is shifted)\n",
    "\n",
    "### Normalization\n",
    "$$X_\\text{new}=\\frac{X}{\\text{norm}(X)}$$\n",
    "\n",
    "Pros:\n",
    "- scale individual samples to their unit norms\n",
    "- can choose l1/l2/max as $\\text{norm}(\\cdot)$\n",
    "\n",
    "### Min-Max Scaling\n",
    "$$\\begin{align*}\n",
    "&x\\in[0,1]: &&X_\\text{new}=\\frac{X-\\min{(X)}}{\\max{(X)}-\\min{(X)}}\\\\\n",
    "&x\\in[\\min,\\max]: &&X_\\text{new}=\\frac{X-\\min{(X)}}{\\max{(X)}-\\min{(X)}}(\\text{max}-\\text{min})+\\text{min}\n",
    "\\end{align*}$$\n",
    "\n",
    "Pros:\n",
    "- can scale each $ x_i $ into a range of your choice\n",
    "\n",
    "Cons:\n",
    "- highly sensitive to outliers (because they greatly impact empirical min & max)\n",
    "- destroy sparsity (because center is shifted)\n",
    "\n",
    "### Max-Abs Scaling\n",
    "$$X_\\text{new}=\\frac{X}{\\max{(|X|)}}$$\n",
    "\n",
    "Pros:\n",
    "- preserve signs of each $ x_i $.- preserve sparsity\n",
    "- scale each $x_i$  into a range of $[-1,1]$ ($[-1,0)$ for neg entries, $(0,1]$ for pos entries)\n",
    "\n",
    "Cons:\n",
    "- highly sensitive to outliers\n",
    "\n",
    "### Robust Scaling\n",
    "$$X_\\text{new}=\\frac{X-\\text{med}(X)}{Q_{75\\%}(X)-Q_{25\\%}(X)}$$\n",
    "\n",
    "Pros:\n",
    "- robust to outliers\n",
    "\n",
    "Cons:\n",
    "- destroy sparsity (because center is shifted)\n",
    "\n",
    "### Quantile Transform\n",
    "- Original form: $X_\\text{new}=Q^{-1}(F(X))$\n",
    "    - $Q^{-1}$: quantile func (i.e., PPF [percent-point func], inverse of CDF)\n",
    "    - $F$: empirical CDF\n",
    "- Uniform outputs: $X_\\text{new}=F_U^{-1}(F(X))\\in[0,1]$\n",
    "- Gaussian outputs: $X_\\text{new}=F_N^{-1}(F(X))\\sim N(0,1)$\n",
    "\n",
    "Pros:\n",
    "- robust to outliers (literally collapse them)\n",
    "\n",
    "Cons:\n",
    "- distort linear correlations between diff features\n",
    "- only work well with sufficiently large #samples\n",
    "\n",
    "\n",
    "### Power Transform\n",
    "- Yeo-Johnson Transform\n",
    "    $$\n",
    "    \\mathbf{x}_i^{(\\lambda)}=\\begin{cases}\n",
    "    \\frac{(\\mathbf{x}_i+1)^\\lambda-1}{\\lambda} & \\text{if }\\lambda\\neq0,\\mathbf{x}_i\\geq0 \\\\\n",
    "    \\ln{(\\mathbf{x}_i+1)}                      & \\text{if }\\lambda=0,\\mathbf{x}_i\\geq0 \\\\\n",
    "    \\frac{1-(1-\\mathbf{x}_i)^{2-\\lambda}}{2-\\lambda} & \\text{if }\\lambda\\neq2,\\mathbf{x}_i<0 \\\\\n",
    "    -\\ln{(1-\\mathbf{x}_i)}                           & \\text{if }\\lambda=2,\\mathbf{x}_i<0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - $ \\lambda $: determined by MLE\n",
    "\n",
    "- Box-Cox Transform\n",
    "    $$\n",
    "    \\mathbf{x}_i^{(\\lambda)}=\\begin{cases}\n",
    "    \\frac{\\mathbf{x}_i^\\lambda-1}{\\lambda} & \\text{if }\\lambda\\neq0 \\\\\n",
    "    \\ln{(\\mathbf{x}_i)} & \\text{if }\\lambda=0\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - only applicable when $ \\mathbf{x}_i>0 $\n",
    "    \n",
    "Pros:\n",
    "- map any data to Gaussian distribution (i.e., stabilize variance and minimize skewness)\n",
    "- useful against heteroskedasticity (i.e., non-const variance).\n",
    "- Sklearn's PowerTransformer converts data to $ N(0,1) $ by default.\n",
    "\n",
    "Cons:\n",
    "- distort linear correlations between diff features\n",
    "\n",
    "### Categorical features\n",
    "- **One-Hot Encoding**: convert each category into a 0-1 feature (excluding a dummy)\n",
    "    - better for nominal data (i.e., no inherent order among categories)\n",
    "- **Label Encoding**: convert each category into a numerical label\n",
    "    - better for ordinal data (i.e., inherent order among categories)\n",
    "\n",
    "\n",
    "\n",
    "## Data Loading\n",
    "NOTE: Loading is significantly more complex IRL compared to school projects.\n",
    "\n",
    "Procedure:\n",
    "1. Choose storage:\n",
    "    - **Databases**: SQL/Relational (only structured data), NoSQL (better for unstructured data)\n",
    "    - **Data Warehouses**: best for analytical tasks\n",
    "    - **Data Lakes**: raw storage of big data from various sources\n",
    "    - **Cloud Storage**\n",
    "2. Validate: check schema, data quality, integrity, etc.\n",
    "3. Format: encoding, batching (for big data), raw saving\n",
    "4. Load:\n",
    "    - **Bulk loading**: load data in large chunks\n",
    "        - minimize logging & transaction overhead\n",
    "        - require system downtime\n",
    "    - **Incremental loading**: load data in small increments\n",
    "        - use timestamps/logs to track changes\n",
    "        - minimize system disruption\n",
    "        - best for real-time data processing\n",
    "    - **Streaming**: load data continuously in real time\n",
    "5. Optimize (i.e., reduce data volume for faster execution)\n",
    "    - [**Indexing**](#indexing): set a primary/secondary index on features with unique/repeated values to retrieve them faster\n",
    "        - best for tables with low data churn (i.e., with fewer inserts/deletes/updates)\n",
    "    - [**Partitioning**](#sharding-horizontal-partitioning): divide a database/table into distinct sections/tables to query them independently\n",
    "        - best for storing older records\n",
    "    - [**Parallel Processing**](#parallel-processing)\n",
    "6. Handle errors\n",
    "7. Handle security: encryption, access control, etc.\n",
    "8. Verify: audit using test queries, reconcile loaded data with source data, etc.\n",
    "\n",
    "### Indexing\n",
    "Why: faster retrieval\n",
    "\n",
    "What: create quick lookup paths to access the data\n",
    "\n",
    "How: Each Index stores values of specific columns & the location of corresponding rows. Indices are stored as B trees/B+ trees.\n",
    "- **B tree**: balanced tree where each node contains multiple keys sorted in order & pointers to child nodes\n",
    "- **B+ tree**: B tree & all records are stored at leaf level with leaf nodes linked to one another\n",
    "\n",
    "Types:\n",
    "- **Single-column Indexes**: created on only one column; used when queries frequently access/filter based on that specific column\n",
    "- **Composite Indexes** (Multi-column Indexes): created on two/more columns; used when queries frequently filter/sort based on these columns in combination\n",
    "- **Unique Indexes**: ensure all index values are unique; used as a primary key\n",
    "- **Full-text Indexes**: allow complex queries on unstructured texts (e.g., search engines, document search)\n",
    "- **Spatial Indexes**: used when queries do geospatial operations\n",
    "\n",
    "Pros:\n",
    "- fast retrieval\n",
    "- auto sort (useful for fetching & presenting data)\n",
    "- high time efficiency (reduce #disk accesses required during queries)\n",
    "\n",
    "Cons:\n",
    "- low space efficiency (require additional space to store indices)\n",
    "- high maintenance complexity\n",
    "\n",
    "### Sharding (horizontal partitioning)\n",
    "Why: scale databases horizontally\n",
    "\n",
    "What: distribute data across multiple servers/locations, where each shard is an independent database and all shards form a single logical database\n",
    "\n",
    "How: use a specific (set of) columns as a **shard key** that determines how the data is distributed across the shards\n",
    "- **Hash-based**: use a hash function to determine which shard will store a given data row; useful for even data division\n",
    "- **Range-based**: use ranges of shard key values to divide data; useful for numerical data division\n",
    "- **List-based**: use predefined lists of shard keys to divide; useful for categorical data division\n",
    "\n",
    "Where: web apps, real-time analytics, game/media services, etc.\n",
    "\n",
    "Pros:\n",
    "- horizontal scalability\n",
    "- high availability\n",
    "\n",
    "Cons:\n",
    "- implementation complexity\n",
    "- high maintenance complexity\n",
    "\n",
    "### Parallel Processing\n",
    "Concepts:\n",
    "- **Core**: A processor has multiple cores, each of which can execute instructions independently.\n",
    "- **Thread**: A core can run multiple threads, each of which can execute sub-tasks independently.\n",
    "- **Memory**:\n",
    "    - Shared memory architectures allow multiple cores to access the same memory space.\n",
    "    - Distributed memory architectures require communication between cores.\n",
    "\n",
    "Types:\n",
    "- **Data Parallelism**: divide data into smaller chunks & processing each chunk simultaneously on different cores\n",
    "- **Task Parallelism**: execute different tasks of one program in parallel\n",
    "\n",
    "Methods:\n",
    "- **Multithreading**: C/C++ OpenMP, Python threading libraries, etc.\n",
    "- **GPGPU** (General-Purpose computing on Graphics Processing Units): use CUDA and OpenCL to perform highly parallelized computing more efficiently than CPUs\n",
    "\n",
    "### Distributed Computing\n",
    "NOTE: Distributed Computing $\\neq$ Parallel Processing\n",
    "- Parallel processing runs on a single computer.\n",
    "- Distributed computing runs on multiple independent computers (i.e., nodes).\n",
    "\n",
    "Concepts:\n",
    "- **Networks**: to connect multiple computers\n",
    "- **Horizontal scalability**: the more machines the better performance (unless beyond capacity)\n",
    "- **Fault Tolerance**: handle failures of individual nodes / network components without affecting the overall task\n",
    "\n",
    "Methods:\n",
    "- **MapReduce**: process big data with a distributed algorithm on a cluster\n",
    "    - Map: filter & sort data\n",
    "    - Reduce: perform a summary operation\n",
    "- **Distributed Databases**: store data across multiple physical locations BUT appear as a single database to users\n",
    "- **Load Balancing**: distribute workloads evenly across all nodes to maximize resource usage & minimize response time"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}