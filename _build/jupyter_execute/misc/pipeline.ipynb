{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c4c1da",
   "metadata": {},
   "source": [
    "# Overview\n",
    "An AI project consists of 4 parts:\n",
    "\n",
    "```{image} ../images/ml/system_design_flow.png\n",
    ":align: center\n",
    "```\n",
    "\n",
    "## Problem\n",
    "1. **Overview**: What problem? Why? What inputs & outputs? (rough idea)\n",
    "2. **Scope**: What constraints? \n",
    "    - **Data constraints**: #samples, #features, feature types, etc.\n",
    "    - **Model constraints**: priority (performance/quality), model type (single general/multiple specific), interpretability, retrainability, etc.\n",
    "    - **Resource constraints**: time (training, inference, project duration, etc.), computation (training, inference, local/cloud, etc.)\n",
    "3. **Evaluation**: Is the problem solved? How do we know?\n",
    "    - **Auto metrics**: offline (MSE, P/R/F1, etc.), online (usage time, usage frequency, click rate, etc.)\n",
    "    - **Human metrics**: user interaction, recent reports, company intention for users, personalization, etc.\n",
    "\n",
    "## Data\n",
    "1. **Type**\n",
    "    - **Features**: user, content, context, etc.\n",
    "    - **Targets**: explicit (direct indicators), implicit (indirect indicators)\n",
    "2. **Availability**\n",
    "    - **Available/Unavailable**: What is available/unavailable? How much is available/unavailable? \n",
    "    - **Annotation**: Are they Annotated? How good are the annotations? How expensive to annotate the rest? How to resolve annotators' disagreements? Is auto-annotation feasible (e.g., ChatGPT, rule-based generation, etc.)?\n",
    "    - **Privacy**: What user data can we access? How do we obtain them? Can we use online/periodic data? Do we need anonymity?\n",
    "    - **Logistics**: Where are the data (local/cloud)? What data structures? How big? What biases in it?\n",
    "3. [**Processing (ETL)**](../ml/data.md)\n",
    "4. [**Feature Engineering**](../ml/unsupervised.md)\n",
    "\n",
    "## Modeling\n",
    "NOTE: For each model, specify:\n",
    "- Why (Motivation)\n",
    "- What (Functionality)\n",
    "- How (Objective & Optimization)\n",
    "- When (Assumptions if any)\n",
    "- Pros & Cons\n",
    "\n",
    "Procedure:\n",
    "1. Baseline ~~model~~: stats (mean, median, mode), random benchmarks, etc.\n",
    "2. Easy model\n",
    "3. Hard model\n",
    "4. Experiment, Evaluation & Ablation Study\n",
    "\n",
    "## Production\n",
    "NOTE: Production $\\neq$ Experiment. Performance degrades in production because of uncertainty:\n",
    "- **Data Drift**: production data $\\neq$ training data $\\rightarrow$ assumption fails\n",
    "- **Feature Drift**: new features / feature transformations $\\rightarrow$ feature engineering pipeline fails\n",
    "- **Concept Drift**: Relationship between features and target variable can change over time, especially in a dynamic environment.\n",
    "- **Data Quality**: missing values, outliers, noisy data, etc.\n",
    "- **Model Versioning**: mismatches between R&D models & deployed models\n",
    "- **Scaling & Latency**: Models IRL need to handle a significantly larger volume of data and to respond significantly faster to enormous requests.\n",
    "- **Ethics**: adversarial attacks, privacy concerns, regulatory compliance, interpretability, etc.\n",
    "- **Others**: Even if everything is flawless, a random error may just appear for no reason (e.g., network issues)\n",
    "\n",
    "Consider the following factors for production:\n",
    "1. **Inference location**:\n",
    "    - **local** (phone/PC): high memory/storage usage, low latency\n",
    "    - **server** (ours/cloud): low memory/storage usage, high latency, privacy concerns\n",
    "2. **Feature serving**:\n",
    "    - **batch features** should be handled offline & served online\n",
    "        - need daily/weekly jobs for auto data generation/collection\n",
    "    - **real-time features** should be handled & served at request time (scalability & latency = priority)\n",
    "        - need a feature store to look up features at serve time\n",
    "        - caching may be necessary\n",
    "3. **Performance Monitoring**: errors, latency, biases, data drift, CPU load, memory, etc.\n",
    "4. [**Online A/B Testing**](#online-a-b-testing)\n",
    "5. **Retrain Frequency**\n",
    "\n",
    "\n",
    "\n",
    "# Concepts\n",
    "## Types of ML\n",
    "- Supervision\n",
    "    - **Supervised** (labeled): learn a mapping from input data to output labels\n",
    "    - **Unsupervised** (unlabeled): find patterns/structures/relationships in data with no labeled output\n",
    "    - **Semi-supervised** (partially labeled): use labeled data to guide learning, use unlabeled data to uncover patterns or improve model performance\n",
    "    - **Active** (continuously labeled): add most informative samples for labeling in an iterative and adaptive manner\n",
    "- Parameterization\n",
    "    - **Parametric**: estimate trainable params from data\n",
    "        - Assume prior functional form/shape of data distribution\n",
    "        - Used when we have strong prior knowledge\n",
    "        - Pros: high interpretability, low data requirement, high computational efficiency\n",
    "    - **Nonparametric**: learn patterns based on data alone\n",
    "        - Assume nothing\n",
    "        - Used in EDA / when we have unknown data distribution\n",
    "        - Pros: high versatility\n",
    "- Prediction\n",
    "    - **Generative**: model $p(x,y)$\n",
    "        - Pros: easy to fit, handle missing features, handle unlabeled data, fit classes separately, better generalization\n",
    "        - Cons: low computational efficiency, sensitive to incorrect assumptions\n",
    "    - **Discriminative**: model $p(y|x)$\n",
    "        - Pros: high accuracy, allow preprocessing, provide calibrated probability estimates, robust to irrelevant features\n",
    "        - Cons: data-dependent, limited application scope\n",
    "- **Ensemble**: combine a bunch of smaller models together for prediction\n",
    "    - Types:\n",
    "        - **Voting**: majority vote (cls) / average vote (reg)\n",
    "        - **Bagging**: train multiple models with bootstrapped subsets of the same data\n",
    "        - **Boosting**: train sequential models where each new model focuses on the samples that the previous models got wrong. \n",
    "        - **Stacking**: train a meta-model which takes the predictions of multiple base models as input and makes the final prediction as output.\n",
    "    - Performance improvement (relative to single models):\n",
    "        - **Variance Reduction**: average out different errors on different subsets $\\rightarrow$ higher accuracy & stability\n",
    "        - **Generalization**: reduce bias $\\rightarrow$ better generalization\n",
    "        - **Diversity**: each model has its own unique strength\n",
    "        - **Robustness to Noise & Outliers**: they are averaged out\n",
    "\n",
    "## Occam's Razor\n",
    "\"Simpler solutions are better.\"\n",
    "- Why better?\n",
    "    - higher interpretability, fewer computational resources, faster training and easiness to debug/retrain\n",
    "- Why does it matter?\n",
    "    - **Model Selection**: If a simpler model reaches comparable performance as a complex model, the simpler model wins.\n",
    "    - **Feature Selection**: If a smaller feature set reaches comparable performance as a larger feature set, the smaller set wins.\n",
    "    - **Generalization**: Overfitting is a core issue in ML. Simpler configs for hyperparameters are best start points to reduce overfitting and lead to better generalization for unseen data.\n",
    "    - **Ensemble methods**: Aggregating simpler models reaches better performance than complex models in many cases.\n",
    "- Why deep learning if Occam's Razor holds?\n",
    "    - **Big Data**: Complex structures work better with larger data\n",
    "    - **Computational Resources**: Thx Nvidia\n",
    "    - **Transformer**: Thx Google\n",
    "    - **Transfer Learning**: We can pretrain a gigantic general-purpose model and finetune it on specialized tasks to reach significantly better performance\n",
    "\n",
    "## Online A/B Testing\n",
    "1. **Define Objective**: improving click-through rates, increasing sign-up rates, etc.\n",
    "    - **Significance level** $(\\alpha)$: threshold of whether the observed difference between control & treatment is statistically significant\n",
    "        - $\\alpha=P(FP)$ (i.e., Type I error): probability of rejecting $H_0$ when it is true.\n",
    "        - Common values: 0.05 and 0.01\n",
    "        - A lower $\\alpha$ makes it more challenging to detect difference.\n",
    "    - **Power** $(1-\\beta)$: probability of rejecting $H_0$ when it is false (i.e., the ability to detect a meaningful difference when it exists)\n",
    "        - $\\beta=P(FN) (i.e., Type II error): probability of not rejecting $H_0$ when it is false.\n",
    "        - Common value: 80%\n",
    "        - A higher power requires larger sample sizes to achieve.\n",
    "2. **Create Variations**: generate 2/more versions of the element we want to test\n",
    "    - e.g., 2 different designs of a button: blue round button $\\rightarrow$ **control group** A; green square button $\\rightarrow$ **treatment group** B\n",
    "3. **Calculate Traffic**: calculate required sample size per variation \n",
    "$$m=2\\times\\left(\\frac{Z_{\\frac{\\alpha}{2}}+Z_{\\beta}}{\\text{MDE}}\\right)^2\\times p(1-p)$$\n",
    "    - $p$ (**Baseline conversion rate**): the occurrence rate of a desired event in the control group\n",
    "    - $\\text{MDE}$ (**Minimum Detectable Effect**): smallest difference in the conversion rate that we want to be able to detect as statistically significant\n",
    "        - e.g., if $p=10\\%$ and we want a minimum improvement of $2\\%$, then $\\text{MDE}=20\\%$\n",
    "        - Both groups should be statistically similar in terms of characteristics for a fair comparison\n",
    "4. **Splitting**: randomly assign users into control & treatment groups \n",
    "    - **User-level**: The user consistently experience the same variation throughout their entire interaction.\n",
    "        - Pros: useful when the tested variations are expected to have a long-lasting impact on user experience, reducing potential biases or confusion due to inconsistency\n",
    "    - **Request-level**: The system randomly determines the variation to be shown at each request made by the user, regardless of previous assignments.\n",
    "        - Pros: useful for measuring immediate or session-specific effects of the tested variations, allowing us to capture any potential context-dependent or short-term effects\n",
    "5. **Measurement & Analysis**: \n",
    "    1. Track & record user interactions, events, or conversions for both the control and treatment groups.\n",
    "    2. Compare the performance of control & treatment groups using statistical analysis.\n",
    "    3. Determine if there is a statistically significant difference in the metrics we are measuring between the tested variations."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}