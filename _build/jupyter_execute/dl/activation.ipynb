{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba883cd",
   "metadata": {},
   "source": [
    "# Activation\n",
    "An activation function adds nonlinearity to the output of a layer (linear in most cases) to enhance complexity.\n",
    "\n",
    "[ReLU](#relu) and [Softmax](#softmax) are SOTA.\n",
    "\n",
    "Notations:\n",
    "- $ z $: input (element-wise)\n",
    "\n",
    "## Binary-like\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "-  $ \\sigma(z)\\in(0,1)$ and $\\sigma(0)=0.5 $.\n",
    "\n",
    "Pros:\n",
    "-  imitation of the firing rate of a neuron, 0 if too negative and 1 if too positive.\n",
    "-  smooth gradient.\n",
    "\n",
    "Cons: \n",
    "-  vanishing gradient: gradients rapidly shrink to 0 along backprop as long as any input is too positive or too negative.\n",
    "-  non-zero centric bias $ \\rightarrow $ non-zero mean activations.\n",
    "-  computationally expensive.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "$$\n",
    "\\tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "- $ \\tanh(z)\\in(-1,1)$ and $\\tanh(0)=0 $.\n",
    "    \n",
    "Pros: \n",
    "- zero-centered\n",
    "- imitation of the firing rate of a neuron, -1 if too negative and 1 if too positive.\n",
    "- smooth gradient.\n",
    "    \n",
    "Cons:\n",
    "- vanishing gradient.\n",
    "- computationally expensive.\n",
    "\n",
    "## Linear Units (Rectified)\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(z)=\\max{(0,z)}\n",
    "$$\n",
    "\n",
    "Name: Rectified Linear Unit\n",
    "\n",
    "Idea:\n",
    "- convert negative linear outputs to 0.\n",
    "    \n",
    "Pros:\n",
    "- no vanishing gradient\n",
    "- activate fewer neurons\n",
    "- much less computationally expensive compared to sigmoid and tanh.\n",
    "    \n",
    "Cons:\n",
    "- dying ReLU: if most inputs are negative, then most neurons output 0 $ \\rightarrow$ no gradient for such neurons $\\rightarrow$ no param update $\\rightarrow $ they die. (NOTE: A SOLVABLE DISADVANTAGE)\n",
    "\n",
    "    - Cause 1: high learning rate $ \\rightarrow$ too much subtraction in param update $\\rightarrow$ weight too negative $\\rightarrow $ input for neuron too negative.\n",
    "    - Cause 2: bias too negative $ \\rightarrow $ input for neuron too negative.\n",
    "\n",
    "-  activation explosion as $ z\\rightarrow\\infty $. (NOTE: NOT A SEVERE DISADVANTAGE SO FAR)\n",
    "    \n",
    "\n",
    "### LReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{LReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Leaky Rectified Linear Unit\n",
    "\n",
    "Params: \n",
    "- $ \\alpha\\in(0,1) $: hyperparam (negative slope), default 0.01. \n",
    "    \n",
    "Idea:\n",
    "- scale negative linear outputs by $ \\alpha $.\n",
    "    \n",
    "Pros:\n",
    "- no dying ReLU.\n",
    "    \n",
    "Cons: \n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "### PReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{PReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "Name: Parametric Rectified Linear Unit\n",
    "\n",
    "Params: \n",
    "- $ \\alpha\\in(0,1) $: learnable parameter (negative slope), default 0.25.\n",
    "    \n",
    "Idea:\n",
    "- scale negative linear outputs by a learnable $ \\alpha $.\n",
    "    \n",
    "Pros: \n",
    "- a variable, adaptive parameter learned from data.\n",
    "    \n",
    "Cons: \n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### RReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{RReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Randomized Rectified Linear Unit\n",
    "\n",
    "Params:     \n",
    "- $ \\alpha\\sim\\mathrm{Uniform}(l,u) $: a random number sampled from a uniform distribution.\n",
    "- $ l,u $: hyperparams (lower bound, upper bound)\n",
    "    \n",
    "Idea:\n",
    "- scale negative linear outputs by a random $ \\alpha $.\n",
    "    \n",
    "Pros: \n",
    "- reduce overfitting by randomization.\n",
    "    \n",
    "Cons: \n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "    \n",
    "## Linear Units (Exponential)\n",
    "\n",
    "### ELU\n",
    "\n",
    "$$\n",
    "\\mathrm{ELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "    \n",
    "Idea:\n",
    "- convert negative linear outputs to the non-linear exponential function above.\n",
    "    \n",
    "Pros: \n",
    "- mean unit activation is closer to 0 $ \\rightarrow $ reduce bias shift (i.e., non-zero mean activation is intrinsically a bias for the next layer.)\n",
    "- lower computational complexity compared to batch normalization.\n",
    "- smooth to $ -\\alpha $ slowly with smaller derivatives that decrease forwardprop variation.\n",
    "- faster learning and higher accuracy for image classification in practice.\n",
    "    \n",
    "Cons: \n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "    \n",
    "\n",
    "\n",
    "### SELU\n",
    "\n",
    "$$\n",
    "\\mathrm{SELU}(z)=\\lambda\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Scaled Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.67326.\n",
    "- $ \\lambda $: hyperparam (scale), default 1.05070.\n",
    "    \n",
    "Idea:\n",
    "- scale ELU.\n",
    "    \n",
    "Pros: \n",
    "- self-normalization $ \\rightarrow $ activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance.\n",
    "    \n",
    "Cons:\n",
    "- more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "    \n",
    "\n",
    "\n",
    "### CELU\n",
    "\n",
    "$$\n",
    "\\mathrm{CELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0\\\\\n",
    "\\alpha(e^{\\frac{z}{\\alpha}}-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Continuously Differentiable Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "    \n",
    "Idea:\n",
    "- scale the exponential part of ELU with $ \\frac{1}{\\alpha} $ to make it continuously differentiable.\n",
    "    \n",
    "Pros:\n",
    "- smooth gradient due to continuous differentiability (i.e., $ \\mathrm{CELU}'(0)=1 $).\n",
    "    \n",
    "Cons:\n",
    "- slightly more computationally expensive than ELU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "    \n",
    "## Linear Units (Others)\n",
    "\n",
    "### GELU\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(z)=z*\\Phi(z)=0.5z(1+\\tanh{[\\sqrt{\\frac{2}{\\pi}}(z+0.044715z^3)]})\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Gaussian Error Linear Unit\n",
    "\n",
    "Idea:\n",
    "- weigh each output value by its Gaussian cdf.\n",
    "    \n",
    "Pros: \n",
    "- throw away gate structure and add probabilistic-ish feature to neuron outputs.\n",
    "- seemingly better performance than the ReLU and ELU families, SOTA in transformers.\n",
    "    \n",
    "Cons:    \n",
    "- slightly more computationally expensive than ReLU.\n",
    "- lack of practical testing at the moment.\n",
    "    \n",
    "\n",
    "\n",
    "### SiLU\n",
    "\n",
    "$$\n",
    "\\mathrm{SiLU}(z)=z*\\sigma(z)\n",
    "$$\n",
    "\n",
    "Name: Sigmoid Linear Unit\n",
    "\n",
    "Idea:   \n",
    "- weigh each output value by its sigmoid value.\n",
    "    \n",
    "Pros: \n",
    "- throw away gate structure.\n",
    "- seemingly better performance than the ReLU and ELU families.\n",
    "    \n",
    "Cons: \n",
    "- worse than GELU.\n",
    "    \n",
    "\n",
    "\n",
    "### Softplus\n",
    "\n",
    "$$\n",
    "\\mathrm{softplus}(z)=\\frac{1}{\\beta}\\log{(1+e^{\\beta z})}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- smooth approximation of ReLU.\n",
    "    \n",
    "Pros: \n",
    "- differentiable and thus theoretically better than ReLU. \n",
    "    \n",
    "Cons: \n",
    "- empirically far worse than ReLU in terms of computation and performance.\n",
    "    \n",
    "\n",
    "\n",
    "## Multiclass\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(z_i)=\\frac{\\exp{(z_i)}}{\\sum_j{\\exp{(z_j)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- convert each value $ z_i$ in the output tensor $\\mathbf{z}$ into its corresponding exponential probability s.t. $\\sum_i{\\mathrm{softmax}(z_i)}=1 $.\n",
    "    \n",
    "Pros: \n",
    "- your single best choice for multiclass classification.\n",
    "    \n",
    "Cons: \n",
    "- mutually exclusive classes (i.e., one input can only be classified into one class.)\n",
    "\n",
    "### Softmin\n",
    "\n",
    "$$\n",
    "\\mathrm{softmin}(z_i)=\\mathrm{softmax}(-z_i)=\\frac{\\exp{(-z_i)}}{\\sum_j{\\exp{(-z_j)}}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "- reverse softmax.\n",
    "    \n",
    "Pros: \n",
    "- suitable for multiclass classification.\n",
    "    \n",
    "Cons:\n",
    "- why not softmax."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}