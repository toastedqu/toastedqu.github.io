{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fab275d",
   "metadata": {},
   "source": [
    "# Toolbox\n",
    "This page is a collection of tools that are widely used in model construction and experimentation, including\n",
    "- Regularization\n",
    "- Optimization\n",
    "- Activation\n",
    "\n",
    "## Regularization\n",
    "Regularization reduces overfitting by adding our prior belief onto loss minimization (i.e., MAP estimate). Such prior belief is associated with our expectation of test error.\n",
    "- Regularization makes a model inconsistent, biased, and scale variant.\n",
    "- Regularization requires extra hyperparameter tuning.\n",
    "- Regularization has a weaker effect on the model (params) as the sample size increases, because more info become available from the data.\n",
    "\n",
    "### Early Stopping\n",
    "Idea: stop the training process when no visible improvement is observed on validation data but on training data.\n",
    "\n",
    "Pros: \n",
    "- faster training & higher computational efficiency\n",
    "\n",
    "Cons:\n",
    "- premature stopping -> underfitting\n",
    "- sensitivity to hyperparams\n",
    "- inconsistency in results if stopped at different points\n",
    "\n",
    "### Penalty\n",
    "Idea: add penalty terms in the loss function to force NN weights to be small. (see [Penalty](../../ml/toolbox/#regularization))\n",
    "\n",
    "Pros:\n",
    "- help keep NN simple\n",
    "- improve model robustness\n",
    "\n",
    "Cons:\n",
    "- oversimplification\n",
    "\n",
    "### Data Augmentation\n",
    "Idea: expand training data (mostly used in CV)\n",
    "\n",
    "CV:\n",
    "- Position: rotate, flip, zoom, translate/shift, shear, scale, cut, erase, etc.\n",
    "- Color: brightness, contrast, jittering, noise injection, channel shuffle, grid distortion, etc.\n",
    "\n",
    "Pros:\n",
    "- improve translation invariance\n",
    "- improve model robustness\n",
    "- improve generalization\n",
    "\n",
    "Cons:\n",
    "- high computational cost\n",
    "- may include artifacts\n",
    "\n",
    "### Dropout\n",
    "Idea: randomly drop out some neurons during training.\n",
    "\n",
    "Pros:\n",
    "- ensemble learning effect\n",
    "- improve training computational efficiency\n",
    "- handle correlated neurons\n",
    "- reduce sensitivity to weight initialization\n",
    "\n",
    "Cons:\n",
    "- interference with learning -> slower convergence rate\n",
    "- sensitive to hyperparam (dropout prob)\n",
    "- unnecessary if sufficient training data on simpler NNs\n",
    "\n",
    "### Normalization\n",
    "Idea: normalize inputs to a layer with zero mean and unit variance (across samples or features) (see [Normalization](../layer/#normalization))\n",
    "\n",
    "Pros:\n",
    "- improve convergence & stabilize learning\n",
    "- allow higher learning rates\n",
    "- sequence independence (layer norm)\n",
    "- batch size independence (layer norm)\n",
    "\n",
    "Cons:\n",
    "- less computational efficiency\n",
    "- sequence dependency (batch norm)\n",
    "- batch size dependency (batch norm)\n",
    "\n",
    "\n",
    "\n",
    "## Optimization\n",
    "Optimization means the adjustment of params to minimize/maximize an objective function. In DL, it involves 5 key components:\n",
    "- **Loss Function**: Difference between predicted output $ \\hat{y}$ and actual output $y $.\n",
    "- **Gradient Descent**: Iteratively use loss gradient to update params to reduce loss. While other optimization methods exist, GD and its variations are the best.\n",
    "- **Learning Rate**: Step size taken during each iteration, controlling convergence and stability of GD.\n",
    "- **Epochs**: #times to go through the entire dataset.\n",
    "- **Batch Size**: #samples in a batch, which impacts how often params are updated.\n",
    "\n",
    "### Gradient Descent\n",
    "$$\\begin{align*}\n",
    "&\\text{Basic ver.:}             &&g_t=\\nabla_w\\mathcal{L}(w_{t-1})\\\\\\\\\n",
    "&\\text{L2 regularization ver.:} &&g_t=\\nabla_w\\mathcal{L}(w_{t-1})+\\lambda w_{t-1}\\\\\\\\\n",
    "&\\text{Weight update:} && w_t=w_{t-1}-\\eta g_t\\\\\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Notations:\n",
    "- $ w_t $: param\n",
    "- $ \\eta $: learning rate\n",
    "- $ g_t $: gradient\n",
    "- $ \\mathcal{L} $: loss\n",
    "- $ \\lambda $: L2 penalty weight\n",
    "\n",
    "Types:\n",
    "- **Stochastic GD**: update params after each sample\n",
    "- **Mini-Batch GD**: update params after each mini-batch of samples\n",
    "- **Batch GD**: update params after the entire dataset\n",
    "\n",
    "Pros:\n",
    "- simple\n",
    "\n",
    "Cons:\n",
    "- stuck in local minima or saddle points\n",
    "- sensitive to learning rate\n",
    "\n",
    "### Momentum\n",
    "$$\\begin{align*}\n",
    "&v_t=\\beta v_{t-1}+(1-\\beta)g_t\\\\\\\\\n",
    "&w_t=w_{t-1}-\\eta v_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Notations:\n",
    "- $ \\beta $: momentum weight\n",
    "    - larger $ \\rightarrow $ smoother updates due to more past gradients involved\n",
    "    - typical values: 0.8, 0.9, 0.999\n",
    "\n",
    "Idea: moving average of past gradients\n",
    "\n",
    "Pros:\n",
    "- accelerate convergence\n",
    "- reduce oscillations & noises\n",
    "- escape local minima & saddle points\n",
    "\n",
    "Cons:\n",
    "- sensitive to hyperparams\n",
    "- overshooting: the weight update jumps over the global minimum\n",
    "\n",
    "### NAG\n",
    "$$\\begin{align*}\n",
    "&v_t=\\beta v_{t-1}+\\nabla_w\\mathcal{L}(w_{t-1}-\\beta v_{t-1})\\\\\\\\\n",
    "&w_t=w_{t-1}-\\eta v_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Name: Nesterov Accelerated Gradient\n",
    "\n",
    "Idea: momentum but look ahead to make an informed update\n",
    "\n",
    "Pros:\n",
    "- further accelerate convergence, espeically near minima\n",
    "- further reduce overshooting\n",
    "- more accurate weight updates in rapidly changing regions\n",
    "- improve robustness to hyperparams\n",
    "\n",
    "Cons:\n",
    "- implementation complexity\n",
    "- low computational efficiency\n",
    "- still sensitive to learning rate\n",
    "\n",
    "### AdaGrad\n",
    "$$\\begin{align*}\n",
    "&v_t=v_{t-1}+g_t^2\\\\\\\\\n",
    "&w_t=w_{t-1}-\\frac{\\eta}{\\sqrt{v_t}+\\epsilon}g_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Notations:\n",
    "- $ \\epsilon $: small number to ensure no division by 0.\n",
    "\n",
    "Name: Adaptive Gradient Algorithm\n",
    "\n",
    "Idea: adapt learning rate for each param\n",
    "\n",
    "Pros:\n",
    "- adaptive learning rate -> improve robustness\n",
    "- efficient for sparse data (where some features have larger gradients than others)\n",
    "\n",
    "Cons:\n",
    "- small learning rate for frequently occurring features -> slow convergence or premature stopping\n",
    "\n",
    "### Adadelta\n",
    "$$\\begin{align*}\n",
    "&v_t=\\beta v\\_{t-1}+(1-\\beta)g_t^2\\\\\\\\\n",
    "&\\Delta w_t=-\\frac{\\sqrt{\\Delta w_{t-1}^2+\\epsilon}}{\\sqrt{v_t}+\\epsilon}g_t\\\\\\\\\n",
    "&w_t=w\\_{t-1}+\\Delta w_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Idea: address small learning rate in AdaGrad by using a window of past gradients to normalize updates\n",
    "\n",
    "Pros:\n",
    "- introduced moving average in adagrad to adapt to changes more effectively\n",
    "- no need for learning rate initialization\n",
    "- robust to varying gradients\n",
    "\n",
    "Cons:\n",
    "- Far too complicated\n",
    "\n",
    "### RMSProp\n",
    "$$\\begin{align*}\n",
    "&v_t=\\beta v_{t-1}+(1-\\beta)g_t^2\\\\\\\\\n",
    "&w_t=w_{t-1}-\\frac{\\eta}{\\sqrt{v_t}+\\epsilon}g_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Name: Root Mean Square Propagation\n",
    "\n",
    "Idea: Momentum + AdaGrad\n",
    "\n",
    "Pros:\n",
    "- simple implementation\n",
    "- no accumulation of update history\n",
    "\n",
    "Cons:\n",
    "- worse than Adam\n",
    "\n",
    "### Adam\n",
    "$$\\begin{align*}\n",
    "&m_t=\\beta_1m_{t-1}+(1-\\beta_1)g_t\\\\\\\\\n",
    "&v_t=\\beta_2v_{t-1}+(1-\\beta_2)g_t^2\\\\\\\\\n",
    "&\\hat{m}_t=\\frac{m_t}{1-\\beta_1^t}\\\\\\\\\n",
    "&\\hat{v}_t=\\frac{v_t}{1-\\beta_2^t}\\\\\\\\\n",
    "&w_t=w\\_{t-1}-\\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\hat{m}_t\n",
    "\\end{align*}$$\n",
    "\n",
    "Notations:\n",
    "- $ m_t $: first moment (adaptive gradient)\n",
    "- $ v_t $: second moment (adaptive learning rate)\n",
    "- $ \\hat{m}_t, \\hat{v}_t $: bias-corrected moments\n",
    "\n",
    "Name: Adaptive Moment Estimation\n",
    "\n",
    "Idea: adaptive learning rates for both momentum & gradient\n",
    "\n",
    "Pros:\n",
    "- SOTA\n",
    "- bias correction\n",
    "- extreme adaptivity\n",
    "- extreme convergence speed\n",
    "- extremely robust to noisy or sparse gradients\n",
    "\n",
    "Cons:\n",
    "- sensitive to hyperparams (3 hyperparams to tune)\n",
    "\n",
    "### AdamW\n",
    "$$\n",
    "w_t=w_{t-1}-\\frac{\\eta}{\\sqrt{\\hat{v}_t}+\\epsilon}\\hat{m}_t-\\lambda w\\_{t-1}\n",
    "$$\n",
    "\n",
    "Idea: Adam + Weight Decay\n",
    "\n",
    "Pros:\n",
    "- Well, weight decay so regularization"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}