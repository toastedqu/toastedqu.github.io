{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0659b6a5",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "# Regression\n",
    "## Mean Squared Error (MSE)\n",
    "## Mean Absolute Error (MAE)\n",
    "## Root Mean Squared Error (RMSE)\n",
    "## Mean Absolute Percentage Error (MAPE)\n",
    "## Mean Squared Logarithmic Error (MSLE)\n",
    "## R-squared (Coefficient of Determination)\n",
    "## Huber Loss\n",
    "## Quantile Loss\n",
    "## Log-Cosh Loss\n",
    "\n",
    "# Classification\n",
    "## Confusion Matrix\n",
    "### Accuracy\n",
    "### Precision\n",
    "### Recall\n",
    "### F1 Score\n",
    "### Specificity\n",
    "### Sensitivity\n",
    "### Balanced Accuracy\n",
    "### ROC Curve (Receiver Operating Characteristic)\n",
    "### AUC (Area Under the ROC Curve)\n",
    "### PR Curve (Precision-Recall Curve)\n",
    "### Average Precision\n",
    "\n",
    "## Probabilistic Metrics\n",
    "### Log Loss (Cross-Entropy Loss)\n",
    "### Brier Score\n",
    "\n",
    "## Multiclass\n",
    "### Multiclass Accuracy\n",
    "### Macro-averaged Metrics\n",
    "### Micro-averaged Metrics\n",
    "### Weighted Metrics\n",
    "\n",
    "## Multilabel\n",
    "### Hamming Loss\n",
    "### Subset Accuracy\n",
    "### F1 Score for Multilabel\n",
    "\n",
    "\n",
    "# Information Retrieval\n",
    "## Retrieval\n",
    "### Precision at K\n",
    "### Recall at K\n",
    "### F1 at K\n",
    "\n",
    "## Ranking\n",
    "### Mean Reciprocal Rank (MRR)\n",
    "### Normalized Discounted Cumulative Gain (NDCG)\n",
    "### Hit Rate\n",
    "\n",
    "# Computer Vision\n",
    "## Segmentation\n",
    "### Intersection over Union (IoU)\n",
    "### Dice Coefficient\n",
    "### Pixel Accuracy\n",
    "### Mean Average Precision (mAP) for Segmentation\n",
    "\n",
    "## Object Detection\n",
    "### Mean Average Precision (mAP) for Object Detection\n",
    "### Precision-Recall Curve for Object Detection\n",
    "\n",
    "# Natural Language Processing\n",
    "## BLEU Score (Bilingual Evaluation Understudy)\n",
    "## ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation)\n",
    "## METEOR Score\n",
    "\n",
    "# Interpretability\n",
    "## Permutation Feature Importance\n",
    "## SHAP (SHapley Additive exPlanations)\n",
    "## LIME (Local Interpretable Model-agnostic Explanations)\n",
    "## Integrated Gradients\n",
    "## Grad-CAM (Gradient-weighted Class Activation Mapping)\n",
    "\n",
    "# Model Robustness\n",
    "## Adversarial Testing\n",
    "## Sensitivity Analysis"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}