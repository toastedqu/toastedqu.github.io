---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# Markov Decision Processes (MDP)
## Definition and Elements of MDPs
## Finite MDPs
## State, Action, and Reward Functions
## Policy and Value Functions
### State-Value Function
### Action-Value Function
## Optimality in MDPs
## Bellman Equations

# Dynamic Programming
## Principle of Optimality
## Policy Evaluation
## Policy Improvement
## Policy Iteration
## Value Iteration
## Asynchronous Dynamic Programming

# Monte Carlo Methods
## Monte Carlo Prediction
## Monte Carlo Estimation of Action Values
## Exploring Starts
## Incremental Implementation
## Off-Policy Methods

# Temporal-Difference Learning
## TD Prediction
## Advantages of TD Methods
## SARSA (State-Action-Reward-State-Action)
## Q-learning
## Expected SARSA
## Off-Policy TD Control

# Eligibility Traces
## Introduction to Eligibility Traces
## n-step TD Prediction
## TD(位)
## SARSA(位) and Q(位)
## Forward vs. Backward View of TD(位)

# Planning and Learning with Tabular Methods
## Dyna-Q and Dyna-Q+
## Prioritized Sweeping
## Integrated Planning, Acting, and Learning

# Function Approximation
## Importance of Function Approximation in RL
## Linear Function Approximation
## Nonlinear Function Approximation
## Policy Gradient Methods
### REINFORCE Algorithm
### Actor-Critic Methods
## Bias-Variance Tradeoff