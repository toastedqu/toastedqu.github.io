
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model-based &#8212; AI Handbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'ml/rl';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="AI Handbook - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="AI Handbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dl/layer.html">Layer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/activation.html">Activation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dl/qa.html">Q&amp;A</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Neuroscience</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../neuro/compneuro.html">Computational Neuroscience</a></li>








</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IRL</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../irl/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../irl/data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../irl/coding.html">Coding interview</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fml/rl.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/ml/rl.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model-based</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Model-based</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free">Model-free</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-exploitation-trade-off">Exploration-Exploitation Trade-off</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-difference-learning">Temporal Difference Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0">TD(0)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa">SARSA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo">Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-learning">Deep Q-Learning</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p>RL is not my major focus, so I include its very basics as part of the ML handbook instead of making a separate one.</p>
<p>Some specifications:</p>
<ul class="simple">
<li><p><strong>Model-based vs Model-free</strong>: Model-based RL assumes a model of the environment, while Model-free RL does not.</p></li>
<li><p><strong>On-policy vs Off-policy</strong>: On-policy learning follows the target policy (<span class="math notranslate nohighlight">\( \pi_\text{behavior}=\pi_\text{target}\)</span>), while Off-policy learning follows a different policy from the target policy (<span class="math notranslate nohighlight">\(\pi_\text{behavior}\neq\pi_\text{target} \)</span>).</p></li>
<li><p><strong>Episodic vs Sequential</strong>:</p>
<ul>
<li><p>Episodic environment: a discrete process where RL is divided into a series of <strong>independent</strong> episodes (i.e., epochs). The agent updates its policy AFTER finishing each episode. The goal is to maximize cumulative reward over all episodes, thus <strong>discount factor is usually set to 1</strong>.</p></li>
<li><p>Sequential environment: a continuous process where each current action affects future actions. The agent updates it policy DURING the process. The goal is to maximize the expected future rewards of the current process, thus discount factor matters.</p></li>
</ul>
</li>
</ul>
<p>Overview:</p>
<center>
<img src="/images/rl/overview.png" width="600"/>
</center>
<section class="tex2jax_ignore mathjax_ignore" id="model-based">
<h1>Model-based<a class="headerlink" href="#model-based" title="Link to this heading">#</a></h1>
<section id="markov-decision-process-mdp">
<h2>Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Link to this heading">#</a></h2>
<p>Assumption:</p>
<ul class="simple">
<li><p>Markov Property: <span class="math notranslate nohighlight">\( P(s_{t+1}|s_t,\cdots,s_0)=P(s_{t+1}|s_t) \)</span></p></li>
<li><p>Stationarity: The underlying specification of transition model and reward structure is fixed.</p></li>
</ul>
<p>Specification:</p>
<ul class="simple">
<li><p>State space: <span class="math notranslate nohighlight">\( \mathcal{S} \)</span></p></li>
<li><p>Action space: <span class="math notranslate nohighlight">\( \mathcal{A}(s) \)</span></p></li>
<li><p>Transition probability (i.e., model): <span class="math notranslate nohighlight">\( p(s'|s,a)=\sum_{r\in\mathcal{R}}p(s',r|s,a) \)</span></p></li>
<li><p>Reward: <span class="math notranslate nohighlight">\( r(s,a,s')=\frac{\sum_{r\in\mathcal{R}}rp(s',r|s,a)}{\sum_{r\in\mathcal{R}}p(s',r|s,a)} \)</span></p></li>
<li><p>Discount factor: <span class="math notranslate nohighlight">\( \gamma\in[0,1] \)</span></p></li>
</ul>
<p>Goal: Find policy <span class="math notranslate nohighlight">\( a_t=\pi(s_t) \)</span> to maximize long-term reward:
$<span class="math notranslate nohighlight">\(
G_t=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
\)</span>$</p>
<p>Policy Attributes:</p>
<ul class="simple">
<li><p>Policy (deterministic/stochastic): <span class="math notranslate nohighlight">\( \pi(a|s) \)</span></p></li>
<li><p>Value (i.e., state value):
$<span class="math notranslate nohighlight">\(
V_\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]=\sum_a\pi(a|s)\sum\_{s',r}p(s',r|s,a)[r+\gamma V\_\pi(s')], \forall s\in\mathcal{S}
\)</span>$</p></li>
<li><p>Q-value (i.e., state-action value):
$<span class="math notranslate nohighlight">\(
Q_\pi(s,a)=\mathbb{E}_\pi[G_t|s_t=s,a_t=a]=\sum\_{s',r}p(s',r|s,a)[r+\gamma V\_\pi(s')], \forall s\in\mathcal{S}, \forall a\in\mathcal{A}
\)</span>$</p></li>
<li><p>Optimal Policy (deterministic): <span class="math notranslate nohighlight">\( \pi\_{\*}(s)=\arg\max\_{a}Q\_{\*}(s,a) \)</span></p></li>
<li><p>Optimal Value:
$<span class="math notranslate nohighlight">\(
V_{\*}(s)=\max_\pi V_\pi(s)=\max_a Q_{\*}(s,a)=\max_a \sum_{s',r}p(s',r|s,a)[r+\gamma V_{\*}(s')]
\)</span>$</p></li>
<li><p>Optimal Q-value:
$<span class="math notranslate nohighlight">\(
Q_{\*}(s,a)=\max_\pi Q_\pi(s,a)=\sum_{s',r}p(s',r|s,a)[r+\gamma\max_{a'}Q_{\*}(s')]
\)</span>$</p></li>
</ul>
<p>Dynamic Programming:</p>
<ul class="simple">
<li><p><strong>Policy Evaluation</strong>: compute <span class="math notranslate nohighlight">\( V_\pi\)</span> for input policy <span class="math notranslate nohighlight">\(\pi\)</span> (time complexity: <span class="math notranslate nohighlight">\(O(|\mathcal{S}|^2|\mathcal{A}|) \)</span>)</p>
<ul>
<li><p>Init <span class="math notranslate nohighlight">\( V(s); \pi(s); \epsilon; \Delta=0 \)</span></p></li>
<li><p>Repeat until <span class="math notranslate nohighlight">\( \Delta&lt;\epsilon\)</span> (i.e., convergence: <span class="math notranslate nohighlight">\(\lim_{k\rightarrow\infty}\\{V_k\\}=V_\pi \)</span>):</p>
<ul>
<li><p>For <span class="math notranslate nohighlight">\( s\in\mathcal{S} \)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( V_\text{prev}\leftarrow V(s) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( V(s)\leftarrow \sum_a\pi(a|s)\sum\_{s',r}p(s',r|s,a)[r+\gamma V\_\pi(s')] \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \Delta\leftarrow\max(\Delta,|V_\text{prev}-V(s)|) \)</span></p></li>
</ol>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Policy Improvement</strong>: update policy <span class="math notranslate nohighlight">\( \pi \)</span> (each update guarantees a strictly better policy)</p>
<ul>
<li><p>Init <em>policy_stable=True</em></p></li>
<li><p>For <span class="math notranslate nohighlight">\( s\in\mathcal{S} \)</span>:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( a_\text{prev}\leftarrow\pi(s) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \pi(s)\rightarrow\arg\max_{a\in\mathcal{A}(s)}\sum_{s',r}p(s',r|s,a)[r+\gamma V(s)] \)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\( a_\text{prev}\neq\pi(s) \)</span>, <em>policy_stable=False</em></p></li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Policy Iteration</strong>: Policy Evaluation + Policy Improvement</p>
<ul>
<li><p>Repeat:</p>
<ol class="arabic simple">
<li><p>Policy Evaluation</p></li>
<li><p>Policy Improvement</p></li>
<li><p>If <em>policy_stable=True</em>, return <span class="math notranslate nohighlight">\( \pi \)</span></p></li>
</ol>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-free">
<h1>Model-free<a class="headerlink" href="#model-free" title="Link to this heading">#</a></h1>
<p>in Model-free RL, we have no information about the environment model (i.e., transition probabilities and reward structure). Therefore, we will have to explore on our own.</p>
<section id="exploration-exploitation-trade-off">
<h2>Exploration-Exploitation Trade-off<a class="headerlink" href="#exploration-exploitation-trade-off" title="Link to this heading">#</a></h2>
<p>There is a trade-off between exploring new strategies and exploiting current knowledge of the environment. Finding the perfect balance in various situations is challenging and sometimes impossible.</p>
<p><span class="math notranslate nohighlight">\( \epsilon \)</span>-greedy:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\( \epsilon \)</span>, execute a random action.</p></li>
<li><p>With probability <span class="math notranslate nohighlight">\( 1-\epsilon \)</span>, execute a greedy action.</p></li>
<li><p>A simple annealing schedule: <span class="math notranslate nohighlight">\( \epsilon_t=\frac{n_0}{n_0+\text{visits}(s_t)}\)</span>, where <span class="math notranslate nohighlight">\(n_0 \)</span> is a hyperparam.</p></li>
</ul>
</section>
<section id="temporal-difference-learning">
<h2>Temporal Difference Learning<a class="headerlink" href="#temporal-difference-learning" title="Link to this heading">#</a></h2>
<p>Idea: learn from current predictions rather than waiting till termination. (a weighted average between previous and current values)</p>
<section id="td-0">
<h3>TD(0)<a class="headerlink" href="#td-0" title="Link to this heading">#</a></h3>
<p>Algorithm (TD(0). i.e., one-step look-ahead):</p>
<ul class="simple">
<li><p>Init <span class="math notranslate nohighlight">\( V(s_{\text{end}})=0; V(s); \pi(s); \alpha\in(0,1]\)</span> (<span class="math notranslate nohighlight">\(\forall s\in\mathcal{S} \)</span>)</p></li>
<li><p>Repeat (For each episode):</p>
<ul>
<li><p>Init <span class="math notranslate nohighlight">\( s \)</span></p></li>
<li><p>For step in episode:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( a\leftarrow\pi(s) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( r,s'\leftarrow s,a \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( V(s)\leftarrow V(s)+\alpha[r+\gamma V(s')-V(s)] \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( s\leftarrow s' \)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\( s=s_\text{end} \)</span>: return</p></li>
</ul>
</li>
</ul>
</section>
<section id="sarsa">
<h3>SARSA<a class="headerlink" href="#sarsa" title="Link to this heading">#</a></h3>
<p>Idea: On-policy TD(0) using Q-value. (<span class="math notranslate nohighlight">\( \epsilon \)</span>-greedy for action choice and future evaluation)</p>
<p>Algorithm:</p>
<ul class="simple">
<li><p>Init <span class="math notranslate nohighlight">\( Q(s_{\text{end}},\cdot)=0; Q(s,a)\)</span> (<span class="math notranslate nohighlight">\(\forall s\in\mathcal{S}\ \forall a\in\mathcal{A}(s) \)</span>)</p></li>
<li><p>Repeat (For each episode):</p>
<ul>
<li><p>Init <span class="math notranslate nohighlight">\( s \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( a\leftarrow\pi_Q(s)\)</span>, where <span class="math notranslate nohighlight">\(\pi_Q(s)\)</span> is policy derived from <span class="math notranslate nohighlight">\(Q(s,\cdot)\)</span> (e.g., <span class="math notranslate nohighlight">\(\epsilon \)</span>-greedy)</p></li>
<li><p>For step in episode:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( r,s'\leftarrow s,a \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( a'\leftarrow\pi_Q(s') \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma Q(s',a')-Q(s,a)] \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( s\leftarrow s'; a\leftarrow a' \)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\( s=s_\text{end} \)</span>: return</p></li>
</ul>
</li>
</ul>
</section>
<section id="q-learning">
<h3>Q-Learning<a class="headerlink" href="#q-learning" title="Link to this heading">#</a></h3>
<p>Idea: Off-policy TD(0). (<span class="math notranslate nohighlight">\( \epsilon \)</span>-greedy for action choice, greedy for future evaluation)</p>
<p>Algorithm:</p>
<ul class="simple">
<li><p>Init <span class="math notranslate nohighlight">\( Q(s_{\text{end}},\cdot)=0; Q(s,a)\)</span> (<span class="math notranslate nohighlight">\(\forall s\in\mathcal{S}\ \forall a\in\mathcal{A}(s) \)</span>)</p></li>
<li><p>Repeat (For each episode):</p>
<ul>
<li><p>Init <span class="math notranslate nohighlight">\( s \)</span></p></li>
<li><p>For step in episode:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( a\leftarrow\pi_Q(s)\)</span>, where <span class="math notranslate nohighlight">\(\pi_Q(s)\)</span> is policy derived from <span class="math notranslate nohighlight">\(Q(s,\cdot)\)</span> (e.g., <span class="math notranslate nohighlight">\(\epsilon \)</span>-greedy)</p></li>
<li><p><span class="math notranslate nohighlight">\( r,s'\leftarrow s,a \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( Q(s,a)\leftarrow Q(s,a)+\alpha[r+\gamma\max_{a'}Q(s',a')-Q(s,a)] \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( s\leftarrow s' \)</span></p></li>
</ol>
</li>
<li><p>If <span class="math notranslate nohighlight">\( s=s_\text{end} \)</span>: return</p></li>
</ul>
</li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>Low variance</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>High bias</p></li>
<li><p>Sensitive to initial Q values</p></li>
<li><p>Easy online learning</p></li>
<li><p>Necessary for non-episodic tasks</p></li>
<li><p>Faster convergence than MC on stochastic tasks</p></li>
</ul>
</section>
</section>
<section id="monte-carlo">
<h2>Monte Carlo<a class="headerlink" href="#monte-carlo" title="Link to this heading">#</a></h2>
<p>Idea: Estimate expected reward by sampling.</p>
<p>Algorithm:</p>
<ul class="simple">
<li><p>Repeat:</p>
<ol class="arabic simple">
<li><p>Sample an episode following the current policy (from <span class="math notranslate nohighlight">\( s_0\)</span> to <span class="math notranslate nohighlight">\(s_\text{end}\)</span> or end when <span class="math notranslate nohighlight">\(t=T\)</span>). Obtain a return <span class="math notranslate nohighlight">\(G_t \)</span> of the episode.</p>
<ul>
<li><p>MC uses empirical mean return instead of expected return, starting from <span class="math notranslate nohighlight">\( s_t\)</span> or <span class="math notranslate nohighlight">\((s_t,a_t) \)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\( V_\pi(s_t)\)</span> = average of returns following all the visits to <span class="math notranslate nohighlight">\(s_t \)</span> in a set of episodes</p></li>
<li><p><span class="math notranslate nohighlight">\( Q_\pi(s_t,a_t)\)</span> = average of returns following all the visits to <span class="math notranslate nohighlight">\((s_t,a_t) \)</span> in a set of episodes</p></li>
</ul>
</li>
<li><p>Update policy with average of <span class="math notranslate nohighlight">\( [G_1,\cdots,G_N] \)</span></p></li>
</ol>
</li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>Lower bias compared to Q-learning</p></li>
<li><p>Less sensitive to initial Q values</p></li>
<li><p>Monte Carlo Tree Search is widely used in the most successful game playing methods</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Higher variance compared to Q-learning</p></li>
<li><p>High computational cost for long episodes</p></li>
<li><p>Limited to episodic tasks</p></li>
<li><p>Slower convergence on stochastic tasks</p></li>
</ul>
</section>
<section id="deep-q-learning">
<h2>Deep Q-Learning<a class="headerlink" href="#deep-q-learning" title="Link to this heading">#</a></h2>
<p>Idea: Represent <span class="math notranslate nohighlight">\( Q(s,a) \)</span> by a neural network.</p>
<p>Model:</p>
<ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\( s \)</span></p></li>
<li><p>Output: <span class="math notranslate nohighlight">\( Q(s,:)\)</span> of size <span class="math notranslate nohighlight">\(|\mathcal{A}(s)| \)</span></p></li>
<li><p>Problem: Instability (i.e., rapid changes) in Q function can cause it to diverge</p></li>
<li><p>Solution: Use 2 networks:</p>
<ul>
<li><p>Q-network: regularly updated, provide value for <span class="math notranslate nohighlight">\( Q(s,a) \)</span></p></li>
<li><p>Target network: occasionally updated, provide value for <span class="math notranslate nohighlight">\( Q(s',a') \)</span></p></li>
</ul>
</li>
</ul>
<p>Algorithm (DQN):</p>
<ul class="simple">
<li><p>Init weights <span class="math notranslate nohighlight">\( W\)</span> for NN (i.e., Q function); <span class="math notranslate nohighlight">\(\mathcal{D} \)</span> as replay memory</p></li>
<li><p>Repeat (For each episode):</p>
<ul>
<li><p>Init <span class="math notranslate nohighlight">\( s \)</span></p></li>
<li><p>For step in episode:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\( a\leftarrow\pi_Q(s)\)</span>, where <span class="math notranslate nohighlight">\(\pi_Q(s)\)</span> is policy derived from <span class="math notranslate nohighlight">\(Q(s,\cdot)\)</span> (e.g., <span class="math notranslate nohighlight">\(\epsilon \)</span>-greedy)</p></li>
<li><p><span class="math notranslate nohighlight">\( r,s'\leftarrow s,a \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \mathcal{D}\)</span>.append((<span class="math notranslate nohighlight">\(s,a,r,s' \)</span>))</p></li>
<li><p><span class="math notranslate nohighlight">\( s\leftarrow s' \)</span></p></li>
<li><p>Sample random minibatches of <span class="math notranslate nohighlight">\( \\{(s_i,a_i,r_i,s_{i+1})\\}_{i=1}^{m}\)</span> from <span class="math notranslate nohighlight">\(\mathcal{D} \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( y_i\leftarrow\begin{cases}r_j &amp;\text{ if }s_{i+1}=s_\text{end} \\\\ r_j+\gamma\max_{a'}Q(s_{i+1},a') &amp;\text{ if }s_{i+1}\neq s_\text{end}\end{cases} \)</span></p></li>
<li><p>GD on <span class="math notranslate nohighlight">\( (y_i-Q(s_i,a_i;W))^2 \)</span></p></li>
</ol>
</li>
</ul>
</li>
</ul>
<p>Prediction: <span class="math notranslate nohighlight">\( \pi(s)=\arg\max_a\hat{Q}(s,a) \)</span></p>
<p>Objective: MSE: <span class="math notranslate nohighlight">\( [r+\gamma\max_{a'}Q(s',a')-Q(s,a)]^2 \)</span></p>
<p>Optimization: GD</p>
<p>Pros:</p>
<ul class="simple">
<li><p>Can play certain game(s) better than humans</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Poor generalization to even slightly different games</p></li>
</ul>
<!--
## Hyperparamater Tuning

### Cross Validation

CV: evaluate how the outcomes will generalize to independent datasets.

## Vanishing/Exploding Gradient

**Gradient**: $ \frac{\partial\mathcal{L}}{\partial w}$, specifically on $w $.

**Vanishing**: When backprop towards input layer, the gradients get smaller and smaller and approach zero which eventually leaves the weights of the front layers nearly unchanged. $ \rightarrow $ gradient descent never converges to optimum.
- Causes:
    - Sigmoid or similar activation funcs. They have 0 gradient when abs(input) is large enough.
    - Gradients at the back are consistently less than 1.0. Therefore the chain reaction approaches 0.
- Symptoms:
    - Param at the back change a lot, while params at the front barely change.
    - Some model weights become 0.
    - The model learns very slowly, and training stagnate at very early iterations.

**Exploding**: in some cases, gradients get larger and larger and eventually causes very large weight updates to the front layers $ \rightarrow $ gradient descent diverges.
- Causes:
    - Bad weight initialization. They cause large loss and therefore large gradients.
    - Gradients at the back are consistently larger than 1.0. Therefore the chain reaction approaches $ \infty $.
- Symptoms:
    - Params grow exponentially.
    - Some model weights become NaN.
    - The model learns crazily, and the changes in params/loss make no sense.

Solutions:
- Proper Weight Inits (e.g., Xavier, Glorot, He.)
    - All layer outputs should have equal variance as input samples.
    - All gradients should have equal variance.
- Proper Activation Funcs (e.g., ReLU, LReLU, ELU, SELU, etc.)
    - Gradient = 1 for positive inputs.
- Batch Normalization
    - normalize inputs to ideally $ N(0,1) $ before passing them to the layer.
- Gradient Clipping
    - Clip gradient with max & min thresholds. Any value beyond will be clipped back to the threshold.


<center>

| Model | Type | Accuracy | Speed (train) | Speed (test) | Interpretability | Scale Invariant | 
|:-|:-:|:-:|:-:|:-:|:-:|:-:|
| Linear Regression | Regression | NO | YES | YES | YES | YES w/o regularization<br>NO with regularization |
| Logistic Regression | Classification | NO | YES | YES | YES | YES w/o regularization<br>NO with regularization |
| Naive Bayes | Classification | NO | YES | YES | YES | YES |
| K-Nearest Neighbors | Both | NO | - | YES on small dataset<br>NO on large dataset | YES | NO |
| Decision Tree | Both | NO | YES | YES | YES | YES |
| Linear SVM | Both | NO | YES | YES | YES | NO |
| Kernel SVM | Both | YES | YES on small dataset<br>NO on large dataset | NO | NO | NO |
| Random Forest | Both | YES | NO | NO | NO | YES |
| Boosting | Both | YES | NO | NO | NO | YES |
| Neural Networks | Both | YES | NO | NO | NO | NO |

</center> -->
<!-- 
### Radial Basis Function

$$
\phi_j(\mathbf{x})=\exp{\left(-\frac{||\mathbf{x}-\mu_j||_2^2}{c}\right)}
$$

Steps:
1. Cluster points $ \mu_j $ with k-means clustering.
2. Pick a width $ c=2\sigma^2$ for all the Gaussian pdfs $N(\mu_j,\sigma^2) $ at each cluster.
3. Fit a linear regression.

Usage:
- $ d<n $: dimensionality reduction
- $ d>n $: convert nonlinear problem to linear
- $ d=n $: switch to a dual representation

Pros:

Cons:
- Scale variant.
- Need to find perfect $ c$. Low $c$ leads to overfitting. High $c $ leads to learning nothing (different centroids may cover each other, which is horrible). -->
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Model-based</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free">Model-free</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-exploitation-trade-off">Exploration-Exploitation Trade-off</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-difference-learning">Temporal Difference Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#td-0">TD(0)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa">SARSA</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#q-learning">Q-Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo">Monte Carlo</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-q-learning">Deep Q-Learning</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Renyi Qu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>